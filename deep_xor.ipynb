{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1950dc30-6166-4e7b-86ef-d606c22c9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0220b098-cfde-4ff6-8a94-2ec8e6b54de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f720da3e-1cc4-4225-9cf3-b3b4055a5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x):\n",
    "    delta_x=1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "#     print(\"debug1. intial input variable = \", x)\n",
    "#     print(\"debug2. initial input grad = \", grad)\n",
    "#     print(\"======================================\")\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "#         print(\"debug 3. idx= \", idx, \", x[idx]= \", x[idx])\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "#         print(\"debug4. grad[idx] = \", grad[idx])\n",
    "#         print(\"debug5. grad = \", grad)\n",
    "#         print(\"======================================\")\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3895cb-a668-4c00-8e58-9581146ee8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogicGate:\n",
    "#     def __init__(self, gate_name, xdata, tdata):\n",
    "#         self.name=gate_name\n",
    "#         self.__xdata=xdata\n",
    "#         self.__tdata=tdata\n",
    "        \n",
    "#         self.__xdata=xdata.reshape(4,2)\n",
    "#         self._tdata=tdata.reshape(4,1)\n",
    "        \n",
    "#         self.__w2=np.random.rand(2,6)\n",
    "#         self.__b2=np.random.rand(1)\n",
    "        \n",
    "#         self.__w3=np.random.rand(6,1)\n",
    "#         self.__b3=np.random.rand(1)\n",
    "        \n",
    "#         self.__learning_rate=1e-2\n",
    "        \n",
    "        \n",
    "#     def feed_forward(self):\n",
    "#         delta=1e-7\n",
    "        \n",
    "#         z2=np.dot(self.__xdata, self.__w2)+self.__b2\n",
    "#         a2=sigmoid(z2)\n",
    "        \n",
    "#         z3=np.dot(a2, self.__w3)+self.__b3\n",
    "#         y=sigmoid(z3)\n",
    "        \n",
    "#         return -np.sum(self.__tdata*np.log(y+delta)+(1-self.__tdata)*np.log((1-y)+delta))\n",
    "        \n",
    "#     def train(self):\n",
    "#         f=lambda x: self.__feed_forward()\n",
    "#         print(\"Initial loss func:\", self.__feed_forward())\n",
    "        \n",
    "#         for step in range(10001):\n",
    "#             self.__w2-=self.__learning_rate*numerical_derivative(f,delf.__w2)\n",
    "#             self.__b2-=self.__learning_rate*numerical_derivative(f,delf.__b2)\n",
    "            \n",
    "#             self.__w3-=self.__learning_rate*numerical_derivative(f,delf.__w3)\n",
    "#             self.__b3-=self.__learning_rate*numerical_derivative(f,delf.__b3)\n",
    "            \n",
    "#             if step % 400 == 0:\n",
    "#                 print(\"step:\", step, \", loss_value:\", self.feed_forward())\n",
    "            \n",
    "#     def predict(self, xdata):\n",
    "#         z2=np.dot(self.__xdata, self.__w2)+self.__b2\n",
    "#         a2=sigmoid(z2)\n",
    "        \n",
    "#         z3=np.dot(a2, self.__w3)+self.__b3\n",
    "#         y=sigmoid(z3)\n",
    "#         if y>=0.5: \n",
    "#             result = 1\n",
    "#         else:\n",
    "#             result =0\n",
    "\n",
    "#         return y, result\n",
    "class LogicGate:\n",
    "    def __init__(self, gate_name, xdata, tdata):\n",
    "        self.name=gate_name\n",
    "        self.__xdata=xdata\n",
    "        self.__tdata=tdata\n",
    "        \n",
    "        self.__xdata=xdata.reshape(4,2)\n",
    "        self.__tdata=tdata.reshape(4,1)\n",
    "        \n",
    "        self.__w2=np.random.rand(2,6)\n",
    "        self.__b2=np.random.rand(1)\n",
    "        \n",
    "        self.__w3=np.random.rand(6,1)\n",
    "        self.__b3=np.random.rand(1)\n",
    "        \n",
    "        self.__learning_rate=1e-2\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        delta=1e-7\n",
    "        \n",
    "        z2=np.dot(self.__xdata, self.__w2)+self.__b2\n",
    "        a2=sigmoid(z2)\n",
    "        \n",
    "        z3=np.dot(a2, self.__w3)+self.__b3\n",
    "        y=sigmoid(z3)\n",
    "         \n",
    "        return -np.sum(self.__tdata*np.log(y+delta)+(1-self.__tdata)*np.log((1-y)+delta))\n",
    "    \n",
    "    def train(self):\n",
    "        f=lambda x:self.feed_forward()\n",
    "        print(\"Initial loss func:\", self.feed_forward())\n",
    "        \n",
    "        for step in range(10001):\n",
    "            self.__w2-=self.__learning_rate*numerical_derivative(f, self.__w2)\n",
    "            self.__b2-=self.__learning_rate*numerical_derivative(f, self.__b2)\n",
    "            \n",
    "            self.__w3-=self.__learning_rate*numerical_derivative(f, self.__w3)\n",
    "            self.__b3-=self.__learning_rate*numerical_derivative(f, self.__b3)\n",
    "            \n",
    "            if step % 400==0:\n",
    "                print(\"step:\", step,\", loss value:\", self.feed_forward())\n",
    "                \n",
    "    def predict(self, xdata):\n",
    "        z2=np.dot(xdata, self.__w2)+self.__b2\n",
    "        a2=sigmoid(z2)\n",
    "        \n",
    "        z3=np.dot(a2, self.__w3)+self.__b3\n",
    "        y=sigmoid(z3)\n",
    "        \n",
    "        if y>=0.5:\n",
    "            result=1\n",
    "        else:\n",
    "            result=0\n",
    "        return y, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bb8a56-4464-4ff1-91c2-4e3c81d9d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,0,0,1])\n",
    "tdata1=np.array([0,1,1,1])\n",
    "tdata2=np.array([1,1,1,0])\n",
    "tdata3=np.array([0,1,1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02628508-d0d5-4f01-b9e2-82570b96148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss func: 10.7396111762212\n",
      "step: 0 , loss value: 10.345432752056611\n",
      "step: 400 , loss value: 2.294484588740508\n",
      "step: 800 , loss value: 2.211698990907208\n",
      "step: 1200 , loss value: 2.0565171754912592\n",
      "step: 1600 , loss value: 1.8232571950480907\n",
      "step: 2000 , loss value: 1.625199257077126\n",
      "step: 2400 , loss value: 1.456383175015927\n",
      "step: 2800 , loss value: 1.2884077229157027\n",
      "step: 3200 , loss value: 1.1058464105195571\n",
      "step: 3600 , loss value: 0.9135546976748004\n",
      "step: 4000 , loss value: 0.733286254110823\n",
      "step: 4400 , loss value: 0.583114936338173\n",
      "step: 4800 , loss value: 0.4670517753338014\n",
      "step: 5200 , loss value: 0.38013273169881656\n",
      "step: 5600 , loss value: 0.3152533939031216\n",
      "step: 6000 , loss value: 0.2662825234956666\n",
      "step: 6400 , loss value: 0.2286818825896588\n",
      "step: 6800 , loss value: 0.1992676928978298\n",
      "step: 7200 , loss value: 0.1758354302848477\n",
      "step: 7600 , loss value: 0.15685113600142617\n",
      "step: 8000 , loss value: 0.1412336272649864\n",
      "step: 8400 , loss value: 0.12820850604066156\n",
      "step: 8800 , loss value: 0.11721162511314118\n",
      "step: 9200 , loss value: 0.10782508272435795\n",
      "step: 9600 , loss value: 0.09973434449827881\n",
      "step: 10000 , loss value: 0.09269912059009949\n",
      "Initial loss func: 2.800065551741139\n",
      "step: 0 , loss value: 2.7770950563170573\n",
      "step: 400 , loss value: 1.9400784192087084\n",
      "step: 800 , loss value: 1.613962782164195\n",
      "step: 1200 , loss value: 1.2458859408451566\n",
      "step: 1600 , loss value: 0.9281166502674956\n",
      "step: 2000 , loss value: 0.6853803699534876\n",
      "step: 2400 , loss value: 0.5124774117354066\n",
      "step: 2800 , loss value: 0.39273887414235703\n",
      "step: 3200 , loss value: 0.30954943853832223\n",
      "step: 3600 , loss value: 0.25059147056407954\n",
      "step: 4000 , loss value: 0.20770479288734603\n",
      "step: 4400 , loss value: 0.17566518088504773\n",
      "step: 4800 , loss value: 0.15112422363283415\n",
      "step: 5200 , loss value: 0.1319001062622828\n",
      "step: 5600 , loss value: 0.11653893267484998\n",
      "step: 6000 , loss value: 0.10404841187442164\n",
      "step: 6400 , loss value: 0.09373526323830711\n",
      "step: 6800 , loss value: 0.08510433327113157\n",
      "step: 7200 , loss value: 0.07779471688460671\n",
      "step: 7600 , loss value: 0.07153842853066501\n",
      "step: 8000 , loss value: 0.06613308596224052\n",
      "step: 8400 , loss value: 0.06142348253881046\n",
      "step: 8800 , loss value: 0.05728891401574833\n",
      "step: 9200 , loss value: 0.05363430477808332\n",
      "step: 9600 , loss value: 0.050383889874385315\n",
      "step: 10000 , loss value: 0.04747664664411849\n",
      "Initial loss func: 3.366649005021642\n",
      "step: 0 , loss value: 3.3358780842476192\n",
      "step: 400 , loss value: 2.1804714015950424\n",
      "step: 800 , loss value: 1.8358763600196653\n",
      "step: 1200 , loss value: 1.4550279843675693\n",
      "step: 1600 , loss value: 1.109900204243802\n",
      "step: 2000 , loss value: 0.8212760260360692\n",
      "step: 2400 , loss value: 0.5995299868045488\n",
      "step: 2800 , loss value: 0.44218145598017006\n",
      "step: 3200 , loss value: 0.3346078663025819\n",
      "step: 3600 , loss value: 0.26100628580963003\n",
      "step: 4000 , loss value: 0.20958084839457186\n",
      "step: 4400 , loss value: 0.1726225899146409\n",
      "step: 4800 , loss value: 0.1452842145692994\n",
      "step: 5200 , loss value: 0.12451218365070005\n",
      "step: 5600 , loss value: 0.10834681239142\n",
      "step: 6000 , loss value: 0.0954989978743478\n",
      "step: 6400 , loss value: 0.08509846083815956\n",
      "step: 6800 , loss value: 0.07654273218349553\n",
      "step: 7200 , loss value: 0.06940485730141616\n",
      "step: 7600 , loss value: 0.06337569986567423\n",
      "step: 8000 , loss value: 0.05822701965377471\n",
      "step: 8400 , loss value: 0.05378729736018148\n",
      "step: 8800 , loss value: 0.0499255578946225\n",
      "step: 9200 , loss value: 0.04654032361283203\n",
      "step: 9600 , loss value: 0.04355192710165011\n",
      "step: 10000 , loss value: 0.04089706776318369\n",
      "Initial loss func: 4.396377872617373\n",
      "step: 0 , loss value: 4.296730859525064\n",
      "step: 400 , loss value: 2.7610335571086293\n",
      "step: 800 , loss value: 2.7551183002621173\n",
      "step: 1200 , loss value: 2.747551572247935\n",
      "step: 1600 , loss value: 2.7374312119082154\n",
      "step: 2000 , loss value: 2.723588515819032\n",
      "step: 2400 , loss value: 2.70452421007031\n",
      "step: 2800 , loss value: 2.678365420195784\n",
      "step: 3200 , loss value: 2.642848803887398\n",
      "step: 3600 , loss value: 2.5953322707627438\n",
      "step: 4000 , loss value: 2.5327883546279466\n",
      "step: 4400 , loss value: 2.451653195976699\n",
      "step: 4800 , loss value: 2.34758027730659\n",
      "step: 5200 , loss value: 2.215749466351264\n",
      "step: 5600 , loss value: 2.052338497675493\n",
      "step: 6000 , loss value: 1.8561233547548341\n",
      "step: 6400 , loss value: 1.6308513242950298\n",
      "step: 6800 , loss value: 1.390486396934579\n",
      "step: 7200 , loss value: 1.1582625012418426\n",
      "step: 7600 , loss value: 0.9545771591013809\n",
      "step: 8000 , loss value: 0.7878804835971638\n",
      "step: 8400 , loss value: 0.6563631416538962\n",
      "step: 8800 , loss value: 0.5538126455548817\n",
      "step: 9200 , loss value: 0.47361904659431286\n",
      "step: 9600 , loss value: 0.41028027288159746\n",
      "step: 10000 , loss value: 0.35961092462088995\n"
     ]
    }
   ],
   "source": [
    "and_obj=LogicGate(\"AND\", xdata,tdata)\n",
    "and_obj.train()\n",
    "or_obj=LogicGate(\"OR_GATE\", xdata,tdata1)\n",
    "or_obj.train()\n",
    "nand_obj=LogicGate(\"NAND_GATE\", xdata,tdata2)\n",
    "nand_obj.train()\n",
    "xor_obj=LogicGate(\"XOR_GATE\", xdata,tdata3)\n",
    "xor_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1252fbc9-2d32-4be9-9ddb-f5b46b061093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00363655]), 0) (array([0.0307595]), 0) (array([0.99974371]), 1) (array([0.10338624]), 0)\n",
      "(array([0.02027473]), 0) (array([0.99197634]), 1) (array([0.99088264]), 1) (array([0.92100523]), 1)\n",
      "(array([0.02027522]), 0) (array([0.99195589]), 1) (array([0.99095181]), 1) (array([0.92152263]), 1)\n",
      "(array([0.95304831]), 1) (array([0.99989812]), 1) (array([0.02214374]), 0) (array([0.08283122]), 0)\n"
     ]
    }
   ],
   "source": [
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for data in test_data:\n",
    "    print(and_obj.predict(data),or_obj.predict(data),nand_obj.predict(data),xor_obj.predict(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
