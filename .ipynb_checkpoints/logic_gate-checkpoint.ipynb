{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9675039-d56f-4744-83f8-1f7e37633a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e479565f-73f6-4ebe-a6d6-b1b3251a6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e69cc6-a80d-424d-8bd2-7b6f0d1b3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x):\n",
    "    delta_x=1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "#     print(\"debug1. intial input variable = \", x)\n",
    "#     print(\"debug2. initial input grad = \", grad)\n",
    "#     print(\"======================================\")\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "#         print(\"debug 3. idx= \", idx, \", x[idx]= \", x[idx])\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "#         print(\"debug4. grad[idx] = \", grad[idx])\n",
    "#         print(\"debug5. grad = \", grad)\n",
    "#         print(\"======================================\")\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4c0dd-6c53-450c-a577-8506f738b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGate: #__ 언더바 두개의 의미는 자바에서의 private과 같다\n",
    "    def __init__(self, gate_name,xdata,tdata):\n",
    "        self.name=gate_name\n",
    "        self.__xdata=xdata.reshqpe(4,2)\n",
    "        self.__tdata=tdata.reshape(4,1)\n",
    "        \n",
    "        self.__w=np.random.rand(2,1) #3X1 행렬\n",
    "        self.__b=np.random.rand(1)\n",
    "        \n",
    "        self.__learning_rate=1e-2\n",
    "        \n",
    "    def __loss_func(self):\n",
    "        delta = 1e-7\n",
    "        z=np.dotself.__xdata, self.__w)+self.__b\n",
    "        y=sigmoid(z)\n",
    "        return -np.sum(self.__tdata*np.log(y+delta)+(1-self.__tdata)*np.log(1-y+delta))\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        f= lambda x : self.__loss_func()\n",
    "        print(\"Initial error value=\", self.__loss_func())\n",
    "        for step in range(300001):\n",
    "        self.__w-= self.__learning_rate*numerical_derivative(f,self.__w)\n",
    "        self.__b-= self.__learning_rate*numerical_derivative(f,self.__b)\n",
    "            if(step % 4000== 0):\n",
    "              print(\"step = \",step, \"error value = \", self.__loss_func())\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        z=np.dot(input_data,self.__w)+self.__b\n",
    "        y=sigmoid(z)\n",
    "\n",
    "        if y>0.5: \n",
    "            result = 1\n",
    "        else:\n",
    "            result =0\n",
    "\n",
    "        return y, result\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
